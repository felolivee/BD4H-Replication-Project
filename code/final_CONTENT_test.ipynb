{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felolivee/BD4H-Replication-Project/blob/main/code/final_CONTENT_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUh-ROn5LKmf"
      },
      "source": [
        "## Config and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Mount Google Drive"
      ],
      "metadata": {
        "id": "_mscCrWZKZqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View and modify the working path\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# View current working directory\n",
        "print(\"Current Working *Directory*:\", os.getcwd())\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Change working directory to your file position\n",
        "path = \"/content/gdrive/My Drive/BD4H-project/code\"\n",
        "os.chdir(path)\n",
        "\n",
        "# Confirm the change\n",
        "print(\"Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMOxIapkKApX",
        "outputId": "c3fbb085-f1ac-4904-8a0e-eb07a299f836"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working *Directory*: /content\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Working Directory: /content/gdrive/My Drive/BD4H-project/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KG_Q3ZnlLHdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ff6684-67d3-47c9-bbd5-def6324213ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Config] Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_curve,\n",
        ")\n",
        "from sklearn.metrics import average_precision_score as pr_auc\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want Word2Vec for embeddings:\n",
        "#from gensim.models import Word2Vec\n",
        "import zipfile\n",
        "\n",
        "# Force a specific random seed for reproducibility (optional)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Holds hyperparameters, file paths, and general settings.\n",
        "    In practice, you could store these in a YAML/JSON file.\n",
        "    \"\"\"\n",
        "    # Data paths\n",
        "    dataset_dir = \"../resource\"\n",
        "    zipped_file = os.path.join(dataset_dir, \"S1_Data.zip\")\n",
        "    input_file  = os.path.join(dataset_dir, \"S1_Data.txt\")  # after unzipping\n",
        "\n",
        "\n",
        "    vocab_file = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "    stop_file = os.path.join(dataset_dir, \"stop.txt\")\n",
        "    vocab_pkl = os.path.join(dataset_dir, \"vocab.pkl\")\n",
        "\n",
        "    # PKLs for train, valid, test data\n",
        "    pkl_train_x = os.path.join(dataset_dir, \"X_train.pkl\")\n",
        "    pkl_train_y = os.path.join(dataset_dir, \"Y_train.pkl\")\n",
        "    pkl_val_x   = os.path.join(dataset_dir, \"X_valid.pkl\")\n",
        "    pkl_val_y   = os.path.join(dataset_dir, \"Y_valid.pkl\")\n",
        "    pkl_test_x  = os.path.join(dataset_dir, \"X_test.pkl\")\n",
        "    pkl_test_y  = os.path.join(dataset_dir, \"Y_test.pkl\")\n",
        "\n",
        "    # For building the vocab\n",
        "    rare_word_threshold = 100\n",
        "    stop_word_threshold = 1e4\n",
        "\n",
        "    unknown_index = 1\n",
        "    vocab_size = 490\n",
        "    n_stops = 12  # last 12 are considered \"stop words\"\n",
        "    n_topics = 50\n",
        "    max_visit_len = 300\n",
        "\n",
        "    # Model hyperparams\n",
        "    embed_size = 100\n",
        "    hidden_size = 200\n",
        "\n",
        "    # Training settings\n",
        "    batch_size = 4  # A small batch size example\n",
        "    grad_clip = 5\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10  # Example short run\n",
        "\n",
        "    # Where to save intermediate outputs\n",
        "    theta_dir = \"theta_with_rnnvec\"\n",
        "    results_dir = \"CONTENT_results\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[Config] Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU0aE6IqeB7n"
      },
      "source": [
        "### EXECUTE Config setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TnaJBPF5eAQE"
      },
      "outputs": [],
      "source": [
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup"
      ],
      "metadata": {
        "id": "fYSvC3LvtsNE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fih4kxz0LTNB"
      },
      "source": [
        "###  Utility Helpers (Pickle, Numpy, Vocab, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mFuj04GzLVsY"
      },
      "outputs": [],
      "source": [
        "def save_pkl(path, obj):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_pkl(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = pickle.load(f)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return obj\n",
        "\n",
        "def save_npy(path, arr):\n",
        "    np.save(path, arr)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_npy(path):\n",
        "    arr = np.load(path, allow_pickle=True)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return arr\n",
        "\n",
        "def ensure_data_unzipped(config: Config):\n",
        "    \"\"\"\n",
        "    Checks if the unzipped S1_Data.txt exists. If not, unzips S1_Data.zip.\n",
        "    Sets config.input_file to the unzipped file.\n",
        "    \"\"\"\n",
        "    if os.path.exists(config.input_file):\n",
        "        print(f\"S1_Data.txt already unzipped at: {config.input_file}\")\n",
        "        return\n",
        "    with zipfile.ZipFile(config.zipped_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(config.dataset_dir)\n",
        "    print(f\"Unzipped {config.zipped_file} => {config.dataset_dir}\")\n",
        "\n",
        "def build_vocab(config: Config):\n",
        "    \"\"\"\n",
        "    Creates vocab.txt and stop.txt from S1_Data.txt by filtering.\n",
        "    Index offset so that 'unknown_index' can be used.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    grouped = df.groupby(\"DX_GROUP_DESCRIPTION\").size().reset_index(name=\"SIZE\")\n",
        "\n",
        "    # Filter out rare\n",
        "    grouped = grouped[grouped[\"SIZE\"] > config.rare_word_threshold]\n",
        "\n",
        "    # Sort by frequency ascending\n",
        "    grouped = grouped.sort_values(by=\"SIZE\").reset_index(drop=True)\n",
        "    vocab = grouped[\"DX_GROUP_DESCRIPTION\"]\n",
        "    vocab.index += 2  # offset => index=1 is reserved for unknown\n",
        "\n",
        "    vocab.to_csv(config.vocab_file, sep=\"\\t\", header=False, index=True)\n",
        "    print(\"Number of valid tokens:\", len(vocab))\n",
        "\n",
        "    # Stop words => extremely frequent\n",
        "    stops = grouped[grouped[\"SIZE\"] > config.stop_word_threshold]\n",
        "    stops[\"DX_GROUP_DESCRIPTION\"].to_csv(config.stop_file, sep=\"\\t\", header=False, index=False)\n",
        "\n",
        "def load_vocab_dict(config: Config):\n",
        "    \"\"\"\n",
        "    Reads vocab_file => returns {word: index}, also pickles reverse mapping.\n",
        "    \"\"\"\n",
        "    word_to_index = {}\n",
        "    with open(config.vocab_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            idx_str, token = line.strip().split(\"\\t\")\n",
        "            word_to_index[token] = int(idx_str) - 1\n",
        "\n",
        "    reverse_mapping = {v: k for k, v in word_to_index.items()}\n",
        "    save_pkl(config.vocab_pkl, reverse_mapping)\n",
        "    print(f\"Vocab size: {len(word_to_index)}\")\n",
        "    return word_to_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aL1dloLX-F"
      },
      "source": [
        "### Data Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yrBqRF-MLbO3"
      },
      "outputs": [],
      "source": [
        "def extract_inpatient_events(config: Config):\n",
        "    \"\"\"\n",
        "    Extracts 'INPATIENT HOSPITAL' events => used to mark readmission (1) if next event is within 30 days.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    inpat = df[df[\"SERVICE_LOCATION\"] == \"INPATIENT HOSPITAL\"]\n",
        "    grouped = (inpat.groupby([\"PID\", \"DAY_ID\", \"SERVICE_LOCATION\"])\n",
        "                    .size()\n",
        "                    .reset_index(name=\"COUNT\")\n",
        "                    .sort_values([\"PID\", \"DAY_ID\"], ascending=True)\n",
        "                    .set_index(\"PID\"))\n",
        "    return grouped\n",
        "\n",
        "def convert_format(config: Config, word2idx, inpat_df):\n",
        "    \"\"\"\n",
        "    Goes through S1_Data.txt line by line => builds docs & labels.\n",
        "    docs[i] = [ [codes_on_day1], [codes_on_day2], ... ]\n",
        "    labels[i] = [0/1 for each day]\n",
        "    \"\"\"\n",
        "    def is_readmitted(pid, day):\n",
        "        try:\n",
        "            recs = inpat_df.loc[int(pid)]\n",
        "            if isinstance(recs, pd.Series):\n",
        "                return (int(day) <= recs.DAY_ID < int(day) + 30)\n",
        "            # else a sub-DataFrame\n",
        "            subset = recs.loc[(int(day) <= recs.DAY_ID) & (recs.DAY_ID < int(day) + 30)]\n",
        "            return subset.shape[0] > 0\n",
        "        except KeyError:\n",
        "            return False\n",
        "\n",
        "    docs, labels = [], []\n",
        "    with open(config.input_file, \"r\") as f:\n",
        "        header = f.readline().strip().split(\"\\t\")\n",
        "        col_idx = {h: i for i, h in enumerate(header)}\n",
        "\n",
        "        doc, visit_codes, label_seq = [], [], []\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            return docs, labels\n",
        "\n",
        "        tokens = line.strip().split(\"\\t\")\n",
        "        pid, day_id = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "        label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "        while line:\n",
        "            tokens = line.strip().split(\"\\t\")\n",
        "            c_pid, c_day = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "\n",
        "            if c_pid != pid:\n",
        "                doc.append(visit_codes)\n",
        "                docs.append(doc)\n",
        "                labels.append(label_seq)\n",
        "                # reset\n",
        "                doc, visit_codes, label_seq = [], [], []\n",
        "                pid, day_id = c_pid, c_day\n",
        "                label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "            else:\n",
        "                # same patient, check if new day\n",
        "                if c_day != day_id:\n",
        "                    doc.append(visit_codes)\n",
        "                    visit_codes = []\n",
        "                    day_id = c_day\n",
        "                    label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "            diag_str = tokens[col_idx[\"DX_GROUP_DESCRIPTION\"]]\n",
        "            diag_idx = word2idx.get(diag_str, config.unknown_index)\n",
        "            visit_codes.append(diag_idx)\n",
        "            line = f.readline()\n",
        "\n",
        "        # finalize\n",
        "        doc.append(visit_codes)\n",
        "        docs.append(doc)\n",
        "        labels.append(label_seq)\n",
        "\n",
        "    return docs, labels\n",
        "\n",
        "def split_and_save(config: Config, docs, labels):\n",
        "    \"\"\"\n",
        "    Splits docs/labels => train/valid/test and saves as pkl.\n",
        "    \"\"\"\n",
        "    # adjust these splits as needed\n",
        "    train_end = 2000\n",
        "    val_end = 2500\n",
        "\n",
        "    save_pkl(config.pkl_train_x, docs[:train_end])\n",
        "    save_pkl(config.pkl_train_y, labels[:train_end])\n",
        "\n",
        "    save_pkl(config.pkl_val_x, docs[train_end:val_end])\n",
        "    save_pkl(config.pkl_val_y, labels[train_end:val_end])\n",
        "\n",
        "    save_pkl(config.pkl_test_x, docs[val_end:])\n",
        "    save_pkl(config.pkl_test_y, labels[val_end:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKvHSFILcv8"
      },
      "source": [
        "### PyTorch Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3a7gCxvtLgJt"
      },
      "outputs": [],
      "source": [
        "class PatientVisitsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset that holds (docs, labels) for a split (train/valid/test).\n",
        "    We'll convert them to multi-hot within __getitem__ or in a collate_fn.\n",
        "    \"\"\"\n",
        "    def __init__(self, docs, labels, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.docs = docs\n",
        "        self.labels = labels\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.docs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.docs[idx], self.labels[idx]\n",
        "\n",
        "def multi_hot_collate_fn(batch, vocab_size, max_len):\n",
        "    \"\"\"\n",
        "    Collate function to transform a list of (doc, label) => (x, y, mask).\n",
        "    Each doc is a list of visits.\n",
        "    We'll multi-hot each visit, padding each sample to a fixed length (max_len).\n",
        "    \"\"\"\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # 1) Separate docs and labels.\n",
        "    docs = [b[0] for b in batch]\n",
        "    labels = [b[1] for b in batch]\n",
        "\n",
        "    # 2) Enforce each sample to have exactly max_len visits (truncating if longer,\n",
        "    #    padding with zeros if shorter).\n",
        "    x_array = np.zeros((batch_size, max_len, vocab_size), dtype=np.float32)\n",
        "    y_array = np.ones((batch_size, max_len), dtype=np.float32)\n",
        "    mask_array = np.zeros((batch_size, max_len), dtype=np.float32)\n",
        "\n",
        "    for i, (doc, lab) in enumerate(zip(docs, labels)):\n",
        "        # Use the minimum between the number of visits in the doc and max_len.\n",
        "        seq_len = min(len(doc), max_len)\n",
        "        mask_array[i, :seq_len] = 1\n",
        "        y_array[i, :seq_len] = lab[:seq_len]\n",
        "        for j in range(seq_len):\n",
        "            visit_codes = doc[j]\n",
        "            for code_idx in visit_codes:\n",
        "                # Adjust for 0-indexed coding; your vocabulary indices seem to be 1-indexed.\n",
        "                x_array[i, j, code_idx - 1] = 1\n",
        "\n",
        "    # Convert numpy arrays to torch tensors.\n",
        "    x_tensor = torch.from_numpy(x_array)\n",
        "    y_tensor = torch.from_numpy(y_array)\n",
        "    mask_tensor = torch.from_numpy(mask_array)\n",
        "    return x_tensor, y_tensor, mask_tensor\n",
        "\n",
        "\n",
        "def create_dataloader(docs, labels, config: Config, shuffle=False):\n",
        "    \"\"\"\n",
        "    Convenience method to build a DataLoader from docs/labels.\n",
        "    \"\"\"\n",
        "    dataset = PatientVisitsDataset(docs, labels, config.vocab_size, config.max_visit_len)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=lambda b: multi_hot_collate_fn(b, config.vocab_size, config.max_visit_len),\n",
        "    )\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Py4FclNbtw"
      },
      "source": [
        "### EXECUTE Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nJFXK01cNhXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25596d4-4645-422f-91b9-473c364aad92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S1_Data.txt already unzipped at: ../resource/S1_Data.txt\n",
            "Number of valid tokens: 490\n",
            "Saved: ../resource/vocab.pkl\n",
            "Vocab size: 490\n",
            "Saved: ../resource/X_train.pkl\n",
            "Saved: ../resource/Y_train.pkl\n",
            "Saved: ../resource/X_valid.pkl\n",
            "Saved: ../resource/Y_valid.pkl\n",
            "Saved: ../resource/X_test.pkl\n",
            "Saved: ../resource/Y_test.pkl\n",
            "Loaded: ../resource/X_train.pkl\n",
            "Loaded: ../resource/Y_train.pkl\n",
            "Loaded: ../resource/X_valid.pkl\n",
            "Loaded: ../resource/Y_valid.pkl\n",
            "Loaded: ../resource/X_test.pkl\n",
            "Loaded: ../resource/Y_test.pkl\n"
          ]
        }
      ],
      "source": [
        "# 1) Build vocab & Load it\n",
        "ensure_data_unzipped(config)\n",
        "build_vocab(config)\n",
        "w2i = load_vocab_dict(config)\n",
        "\n",
        "# 2) Extract events => docs => split => pkl\n",
        "inpat_events = extract_inpatient_events(config)\n",
        "docs, labels = convert_format(config, w2i, inpat_events)\n",
        "split_and_save(config, docs, labels)\n",
        "\n",
        "# 3) Load those splits into memory\n",
        "X_train = load_pkl(config.pkl_train_x)\n",
        "Y_train = load_pkl(config.pkl_train_y)\n",
        "\n",
        "X_valid = load_pkl(config.pkl_val_x)\n",
        "Y_valid = load_pkl(config.pkl_val_y)\n",
        "\n",
        "X_test  = load_pkl(config.pkl_test_x)\n",
        "Y_test  = load_pkl(config.pkl_test_y)\n",
        "\n",
        "# 4) Create DataLoaders\n",
        "train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "valid_loader = create_dataloader(X_valid, Y_valid, config, shuffle=False)\n",
        "test_loader  = create_dataloader(X_test,  Y_test,  config, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6umc8Ym6Li_d"
      },
      "source": [
        "### Defining CONTENT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IUAIzhoILlv7"
      },
      "outputs": [],
      "source": [
        "class ThetaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Reparameterization for the topic distribution + KL term.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, n_topics):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.klterm = 0.0\n",
        "        self.theta = None\n",
        "        self.n_topics = n_topics\n",
        "\n",
        "    def forward(self, mu, log_sigma):\n",
        "        eps = torch.randn_like(mu)\n",
        "        z = mu + torch.exp(0.5 * log_sigma) * eps\n",
        "        theta = F.softmax(z, dim=1)\n",
        "        self.theta = theta\n",
        "\n",
        "        # kl\n",
        "        self.klterm = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp())\n",
        "\n",
        "        # Expand => [batch_size, seq_len, n_topics]\n",
        "        expanded_theta = theta.unsqueeze(1).expand(-1, self.max_len, self.n_topics)\n",
        "        return expanded_theta\n",
        "\n",
        "\n",
        "class ContentModel(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN + Topic model:\n",
        "      1) embed => GRU\n",
        "      2) dense => mu, log_sigma => Theta\n",
        "      3) B * Theta => context\n",
        "      4) sum => final prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed_size = config.embed_size\n",
        "        self.hidden_size= config.hidden_size\n",
        "        self.n_topics   = config.n_topics\n",
        "        self.max_len    = config.max_visit_len\n",
        "\n",
        "        self.embed = nn.Linear(self.vocab_size, self.embed_size, bias=False)\n",
        "        self.gru = nn.GRU(self.embed_size, self.hidden_size, batch_first=True)\n",
        "\n",
        "        self.dense1 = nn.Linear(self.vocab_size, self.hidden_size)\n",
        "        self.dense2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.mu = nn.Linear(self.hidden_size, self.n_topics)\n",
        "        self.log_sigma = nn.Linear(self.hidden_size, self.n_topics)\n",
        "\n",
        "        self.B = nn.Linear(self.vocab_size, self.n_topics, bias=False)\n",
        "        self.out_layer = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        self.theta_layer = ThetaLayer(self.max_len, self.n_topics)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x => [batch, seq_len, vocab_size]\n",
        "        embedded = self.embed(x) * mask.unsqueeze(-1)  # [B, T, embed_size]\n",
        "        gru_out, h_n = self.gru(embedded)\n",
        "\n",
        "        # topic\n",
        "        h1 = F.relu(self.dense1(x))  # [B, T, hidden]\n",
        "        h2 = F.relu(self.dense2(h1)) # [B, T, hidden]\n",
        "\n",
        "        avg_h2 = h2.mean(dim=1)      # [B, hidden]\n",
        "        mu_val = self.mu(avg_h2)\n",
        "        log_sigma_val = self.log_sigma(avg_h2)\n",
        "\n",
        "        theta_expanded = self.theta_layer(mu_val, log_sigma_val)  # [B, T, n_topics]\n",
        "\n",
        "        # context\n",
        "        B_out = self.B(x)  # [B, T, n_topics]\n",
        "        context = (B_out * theta_expanded).mean(dim=-1)  # [B, T]\n",
        "\n",
        "        # combine\n",
        "        rnn_scores = self.out_layer(gru_out).squeeze(-1)  # [B, T]\n",
        "        logits = rnn_scores + context\n",
        "        out = torch.sigmoid(logits)\n",
        "        out = out * mask  # zeros for padded positions\n",
        "        out = torch.clamp(out, min=1e-6, max=1 - 1e-6)  # ensure strictly in (0,1)\n",
        "        return out, h_n, self.theta_layer.theta\n",
        "\n",
        "    @property\n",
        "    def kl_term(self):\n",
        "        return self.theta_layer.klterm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S93_mShgLnX3"
      },
      "source": [
        "### Defining Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TAA6PVXgLvU4"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, config: Config):\n",
        "    \"\"\"\n",
        "    Train step over 'loader' => returns average train loss, plus any collected [theta+hidden].\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    batch_count= 0\n",
        "    collector  = []\n",
        "\n",
        "    for x_batch, y_batch, m_batch in loader:\n",
        "        x_batch = x_batch.to(config.device)\n",
        "        y_batch = y_batch.to(config.device)\n",
        "        m_batch = m_batch.to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds, h_n, theta = model(x_batch, m_batch)\n",
        "\n",
        "        bce = F.binary_cross_entropy(preds.view(-1), y_batch.view(-1), reduction=\"sum\")\n",
        "        loss= bce + model.kl_term\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_count+= 1\n",
        "\n",
        "        # store [theta + hidden]\n",
        "        rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "        theta_np= theta.detach().cpu().numpy()\n",
        "        combined= np.concatenate([theta_np, rnn_vec], axis=1)\n",
        "        collector.append(combined)\n",
        "\n",
        "    avg_loss = total_loss / max(batch_count,1)\n",
        "    return avg_loss, collector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Evaluation Functions"
      ],
      "metadata": {
        "id": "S1kvtURJia5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader, config: Config):\n",
        "    \"\"\"\n",
        "    Evaluates on a DataLoader (e.g. test/valid).\n",
        "    Returns (avg_loss, list_of_true, list_of_pred, theta_hidden_collector).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    batch_count= 0\n",
        "    all_true, all_pred = [], []\n",
        "    all_theta_hidden = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch, mask_batch in loader:\n",
        "            x_batch = x_batch.to(config.device)\n",
        "            y_batch = y_batch.to(config.device)\n",
        "            mask_batch = mask_batch.to(config.device)\n",
        "\n",
        "            preds, h_n, theta = model(x_batch, mask_batch)\n",
        "            bce = F.binary_cross_entropy(preds.view(-1), y_batch.view(-1), reduction=\"sum\")\n",
        "            loss= bce + model.kl_term\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # flatten predictions/labels ignoring masked positions\n",
        "            seq_lens = mask_batch.sum(dim=1).cpu().numpy().astype(int)\n",
        "            preds_np = preds.detach().cpu().numpy()\n",
        "            y_np     = y_batch.detach().cpu().numpy()\n",
        "\n",
        "            for i in range(x_batch.shape[0]):\n",
        "                length_i = seq_lens[i]\n",
        "                all_pred.extend(preds_np[i, :length_i])\n",
        "                all_true.extend(y_np[i, :length_i])\n",
        "\n",
        "            # store [theta + hidden]\n",
        "            rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "            theta_np= theta.detach().cpu().numpy()\n",
        "            combined= np.concatenate([theta_np, rnn_vec], axis=1)\n",
        "            all_theta_hidden.append(combined)\n",
        "\n",
        "    avg_loss = total_loss / (batch_count if batch_count else 1)\n",
        "    return avg_loss, all_true, all_pred, all_theta_hidden\n",
        "\n",
        "\n",
        "def compute_metrics(true_vals, pred_vals):\n",
        "    \"\"\"\n",
        "    Returns a dict with AUC, PR-AUC, ACC, Precision, Recall, F1.\n",
        "    \"\"\"\n",
        "    auc_val = roc_auc_score(true_vals, pred_vals)\n",
        "    pr_val  = pr_auc(true_vals, pred_vals)\n",
        "    preds_bin = (np.array(pred_vals) > 0.5).astype(int)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(true_vals, preds_bin, average=\"binary\")\n",
        "    acc_val = accuracy_score(true_vals, preds_bin)\n",
        "    return {\n",
        "        \"auc\": auc_val,\n",
        "        \"prauc\": pr_val,\n",
        "        \"acc\": acc_val,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "2pPWSJdwiZ-P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTIONAL Grid Search"
      ],
      "metadata": {
        "id": "LMybtDIMuUZk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSlY03-eX4i"
      },
      "source": [
        "### Defining Grid Search Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure output directories exist\n",
        "os.makedirs(\"theta_with_rnnvec\", exist_ok=True)\n",
        "os.makedirs(\"CONTENT_results\", exist_ok=True)\n",
        "\n",
        "\n",
        "def grid_search(param_grid):\n",
        "  best_val_metric = 0\n",
        "  best_config = None\n",
        "\n",
        "  for embed_size, hidden_size, batch_size, learning_rate, num_epochs in itertools.product(\n",
        "          param_grid[\"embed_size\"],\n",
        "          param_grid[\"hidden_size\"],\n",
        "          param_grid[\"batch_size\"],\n",
        "          param_grid[\"learning_rate\"],\n",
        "          param_grid[\"num_epochs\"]\n",
        "      ):\n",
        "      config = Config()  # Initialize a new configuration instance\n",
        "      config.embed_size = embed_size\n",
        "      config.hidden_size = hidden_size\n",
        "      config.batch_size = batch_size\n",
        "      config.learning_rate = learning_rate\n",
        "      config.num_epochs = num_epochs\n",
        "\n",
        "      # (Re)create your DataLoaders with the updated batch size if necessary\n",
        "      train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "      valid_loader = create_dataloader(X_valid, Y_valid, config, shuffle=False)\n",
        "\n",
        "      # Build model and optimizer with these settings\n",
        "      model = ContentModel(config).to(config.device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "      train(model, train_loader, optimizer, config)  # Your training function call\n",
        "\n",
        "      best_val_metric_this_run = 0\n",
        "      val_loss, val_true, val_pred, val_theta_collector = evaluate_model(model, valid_loader, config)\n",
        "      val_metrics = compute_metrics(val_true, val_pred)\n",
        "\n",
        "      if val_metrics[\"acc\"] > best_val_metric_this_run:\n",
        "          best_val_metric_this_run = val_metrics[\"acc\"]\n",
        "\n",
        "      # Update the best overall configuration if the current run outperforms previous runs\n",
        "      if best_val_metric_this_run > best_val_metric:\n",
        "          best_val_metric = best_val_metric_this_run\n",
        "          best_config = {\n",
        "              \"embed_size\": embed_size,\n",
        "              \"hidden_size\": hidden_size,\n",
        "              \"batch_size\": batch_size,\n",
        "              \"learning_rate\": learning_rate,\n",
        "              \"num_epochs\": num_epochs\n",
        "          }\n",
        "\n",
        "  return best_val_metric, best_config"
      ],
      "metadata": {
        "id": "PfBVtwQThCVx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing Grid Search"
      ],
      "metadata": {
        "id": "eC-znEQIjXX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import itertools\n",
        "\n",
        "# DEFINE HYPERPARAMETERS YOU WANT TO TRY HERE\n",
        "param_grid = {\n",
        "    \"embed_size\": [100, 150],\n",
        "    \"hidden_size\": [200, 300],\n",
        "    \"batch_size\": [1, 2, 5, 10],\n",
        "    \"learning_rate\": [0.01, 0.001],\n",
        "    \"num_epochs\": [10]\n",
        "}\n",
        "\n",
        "best_val_metric, best_config = grid_search(param_grid)\n",
        "\n",
        "print(\"Best validation metric:\", best_val_metric)\n",
        "print(\"Best hyperparameters:\", best_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kldmUG47g7pM",
        "outputId": "d030c148-f77c-4fcb-8d14-a30705f16a6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation metric: 0.8250213805742211\n",
            "Best hyperparameters: {'embed_size': 150, 'hidden_size': 300, 'batch_size': 2, 'learning_rate': 0.01, 'num_epochs': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update Config to Optimized Hyperparameters for Final Training + Testing"
      ],
      "metadata": {
        "id": "5cGGbhitmbLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to use hardcoded Config, DO NOT RUN THIS!!\n",
        "\n",
        "config.embed_size = best_config['embed_size']\n",
        "config.hidden_size = best_config['hidden_size']\n",
        "config.batch_size = best_config['batch_size']\n",
        "config.learning_rate = best_config['learning_rate']\n",
        "config.num_epochs = best_config['num_epochs']\n",
        "\n",
        "# (Re)create your Training and Testing DataLoaders with the updated batch size if necessary\n",
        "train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "test_loader = create_dataloader(X_test,  Y_test,  config, shuffle=False)"
      ],
      "metadata": {
        "id": "QyIK_XZnml8F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Model"
      ],
      "metadata": {
        "id": "IZz74tMTvKQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing Model Initialization"
      ],
      "metadata": {
        "id": "hAW8MLh6uqCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initliaze your model with the hardcoded/grid-search hyperparam\n",
        "model = ContentModel(config).to(config.device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "Sb9Hy7E1utKu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing Training"
      ],
      "metadata": {
        "id": "3rCD7WZmrCAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(config.num_epochs):\n",
        "  st = time.time()\n",
        "  train_loss, train_theta_collector = train(model, train_loader, optimizer, config)\n",
        "  train_thetas_arr = np.concatenate(train_theta_collector, axis=0)\n",
        "  np.save(os.path.join(config.theta_dir, f\"thetas_train_{epoch}.npy\"), train_thetas_arr)\n",
        "\n",
        "  elapsed = time.time() - st\n",
        "  print(f\"\\nEpoch {epoch+1}/{config.num_epochs} took {elapsed:.2f}s\")\n",
        "  print(f\"  [Train] loss={train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O8ndEhbrAAw",
        "outputId": "3d7413be-5aa7-4c8a-a8bc-5e815c563cd0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/2 took 12.16s\n",
            "  [Train] loss=6170.3356\n",
            "\n",
            "Epoch 2/2 took 11.65s\n",
            "  [Train] loss=6169.6644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub5jcT6Mefqb"
      },
      "source": [
        "### Executing Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DevIGIDCdx79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9c6187-96b3-4a5a-87a5-036cad03c850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test] loss=6147.9422, AUC=0.7563, PR-AUC=0.5668, ACC=0.8178, Precision=0.6465, Recall=0.4102, F1=0.5019\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model on Testing Data\n",
        "test_loss, test_true, test_pred, test_theta_collector = evaluate_model(model, test_loader, config)\n",
        "test_metrics = compute_metrics(test_true, test_pred)\n",
        "\n",
        "# Save the test results\n",
        "test_thetas_arr = np.concatenate(test_theta_collector, axis=0)\n",
        "np.save(os.path.join(config.theta_dir,  f\"thetas_test_final.npy\"), test_thetas_arr)\n",
        "np.save(os.path.join(config.results_dir,f\"test_labels_final.npy\"), np.array(test_true))\n",
        "np.save(os.path.join(config.results_dir,f\"test_preds_final.npy\"),  np.array(test_pred))\n",
        "\n",
        "print(f\"[Test] loss={test_loss:.4f}, AUC={test_metrics['auc']:.4f}, \"\n",
        "      f\"PR-AUC={test_metrics['prauc']:.4f}, ACC={test_metrics['acc']:.4f}, \"\n",
        "      f\"Precision={test_metrics['precision']:.4f}, Recall={test_metrics['recall']:.4f}, \"\n",
        "      f\"F1={test_metrics['f1']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "content-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}