{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUh-ROn5LKmf"
      },
      "source": [
        "## Config and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KG_Q3ZnlLHdY"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../resource/S1_Data.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     30\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mConfig\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Holds hyperparameters, file paths, and general settings.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    In practice, you could store these in a YAML/JSON file.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Data paths\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mConfig\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Unzip the file and set the input_file path\u001b[39;00m\n\u001b[1;32m     40\u001b[0m input_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS1_Data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mS1_Data.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     42\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(dataset_dir)\n\u001b[1;32m     45\u001b[0m vocab_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/zipfile.py:1250\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../resource/S1_Data.zip'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_curve,\n",
        ")\n",
        "from sklearn.metrics import average_precision_score as pr_auc\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you want Word2Vec for embeddings:\n",
        "from gensim.models import Word2Vec\n",
        "import zipfile\n",
        "\n",
        "# Force a specific random seed for reproducibility (optional)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Holds hyperparameters, file paths, and general settings.\n",
        "    In practice, you could store these in a YAML/JSON file.\n",
        "    \"\"\"\n",
        "    # Data paths\n",
        "    dataset_dir = \"../resource\"\n",
        "    # Unzip the file and set the input_file path\n",
        "    input_file = os.path.join(dataset_dir, \"S1_Data.txt\")\n",
        "    with zipfile.ZipFile(os.path.join(dataset_dir, \"S1_Data.zip\"), 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_dir)\n",
        "    \n",
        "\n",
        "    vocab_file = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "    stop_file = os.path.join(dataset_dir, \"stop.txt\")\n",
        "    vocab_pkl = os.path.join(dataset_dir, \"vocab.pkl\")\n",
        "\n",
        "    # PKLs for train, valid, test data\n",
        "    pkl_train_x = os.path.join(dataset_dir, \"X_train.pkl\")\n",
        "    pkl_train_y = os.path.join(dataset_dir, \"Y_train.pkl\")\n",
        "    pkl_val_x   = os.path.join(dataset_dir, \"X_valid.pkl\")\n",
        "    pkl_val_y   = os.path.join(dataset_dir, \"Y_valid.pkl\")\n",
        "    pkl_test_x  = os.path.join(dataset_dir, \"X_test.pkl\")\n",
        "    pkl_test_y  = os.path.join(dataset_dir, \"Y_test.pkl\")\n",
        "\n",
        "    # For building the vocab\n",
        "    rare_word_threshold = 100\n",
        "    stop_word_threshold = 1e4\n",
        "\n",
        "    unknown_index = 1\n",
        "    vocab_size = 490\n",
        "    n_stops = 12  # last 12 are considered \"stop words\"\n",
        "    n_topics = 50\n",
        "    max_visit_len = 300\n",
        "\n",
        "    # Model hyperparams\n",
        "    embed_size = 100\n",
        "    hidden_size = 200\n",
        "    dropout = 1.0\n",
        "\n",
        "    # Training settings\n",
        "    batch_size = 4  # A small batch size example\n",
        "    grad_clip = 5\n",
        "    learning_rate = 1e-3\n",
        "    num_epochs = 2  # Example short run\n",
        "\n",
        "    # Where to save intermediate outputs\n",
        "    theta_dir = \"theta_with_rnnvec\"\n",
        "    results_dir = \"CONTENT_results\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[Config] Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU0aE6IqeB7n"
      },
      "source": [
        "### EXECUTE Config setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TnaJBPF5eAQE"
      },
      "outputs": [],
      "source": [
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fih4kxz0LTNB"
      },
      "source": [
        "###  Utility Helpers (Pickle, Numpy, Vocab, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mFuj04GzLVsY"
      },
      "outputs": [],
      "source": [
        "def save_pkl(path, obj):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_pkl(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = pickle.load(f)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return obj\n",
        "\n",
        "def save_npy(path, arr):\n",
        "    np.save(path, arr)\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "def load_npy(path):\n",
        "    arr = np.load(path, allow_pickle=True)\n",
        "    print(f\"Loaded: {path}\")\n",
        "    return arr\n",
        "\n",
        "def build_vocab(config: Config):\n",
        "    \"\"\"\n",
        "    Creates vocab.txt and stop.txt from S1_Data.txt by filtering.\n",
        "    Index offset so that 'unknown_index' can be used.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    grouped = df.groupby(\"DX_GROUP_DESCRIPTION\").size().reset_index(name=\"SIZE\")\n",
        "\n",
        "    # Filter out rare\n",
        "    grouped = grouped[grouped[\"SIZE\"] > config.rare_word_threshold]\n",
        "\n",
        "    # Sort by frequency ascending\n",
        "    grouped = grouped.sort_values(by=\"SIZE\").reset_index(drop=True)\n",
        "    vocab = grouped[\"DX_GROUP_DESCRIPTION\"]\n",
        "    vocab.index += 2  # offset => index=1 is reserved for unknown\n",
        "\n",
        "    vocab.to_csv(config.vocab_file, sep=\"\\t\", header=False, index=True)\n",
        "    print(\"Number of valid tokens:\", len(vocab))\n",
        "\n",
        "    # Stop words => extremely frequent\n",
        "    stops = grouped[grouped[\"SIZE\"] > config.stop_word_threshold]\n",
        "    stops[\"DX_GROUP_DESCRIPTION\"].to_csv(config.stop_file, sep=\"\\t\", header=False, index=False)\n",
        "\n",
        "def load_vocab_dict(config: Config):\n",
        "    \"\"\"\n",
        "    Reads vocab_file => returns {word: index}, also pickles reverse mapping.\n",
        "    \"\"\"\n",
        "    word_to_index = {}\n",
        "    with open(config.vocab_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            idx_str, token = line.strip().split(\"\\t\")\n",
        "            word_to_index[token] = int(idx_str) - 1\n",
        "\n",
        "    reverse_mapping = {v: k for k, v in word_to_index.items()}\n",
        "    save_pkl(config.vocab_pkl, reverse_mapping)\n",
        "    print(f\"Vocab size: {len(word_to_index)}\")\n",
        "    return word_to_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aL1dloLX-F"
      },
      "source": [
        "### Data Preprocessing and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yrBqRF-MLbO3"
      },
      "outputs": [],
      "source": [
        "def extract_inpatient_events(config: Config):\n",
        "    \"\"\"\n",
        "    Extracts 'INPATIENT HOSPITAL' events => used to mark readmission (1) if next event is within 30 days.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(config.input_file, sep=\"\\t\", header=0)\n",
        "    inpat = df[df[\"SERVICE_LOCATION\"] == \"INPATIENT HOSPITAL\"]\n",
        "    grouped = (inpat.groupby([\"PID\", \"DAY_ID\", \"SERVICE_LOCATION\"])\n",
        "                    .size()\n",
        "                    .reset_index(name=\"COUNT\")\n",
        "                    .sort_values([\"PID\", \"DAY_ID\"], ascending=True)\n",
        "                    .set_index(\"PID\"))\n",
        "    return grouped\n",
        "\n",
        "def convert_format(config: Config, word2idx, inpat_df):\n",
        "    \"\"\"\n",
        "    Goes through S1_Data.txt line by line => builds docs & labels.\n",
        "    docs[i] = [ [codes_on_day1], [codes_on_day2], ... ]\n",
        "    labels[i] = [0/1 for each day]\n",
        "    \"\"\"\n",
        "    def is_readmitted(pid, day):\n",
        "        try:\n",
        "            recs = inpat_df.loc[int(pid)]\n",
        "            if isinstance(recs, pd.Series):\n",
        "                return (int(day) <= recs.DAY_ID < int(day) + 30)\n",
        "            # else a sub-DataFrame\n",
        "            subset = recs.loc[(int(day) <= recs.DAY_ID) & (recs.DAY_ID < int(day) + 30)]\n",
        "            return subset.shape[0] > 0\n",
        "        except KeyError:\n",
        "            return False\n",
        "\n",
        "    docs, labels = [], []\n",
        "    with open(config.input_file, \"r\") as f:\n",
        "        header = f.readline().strip().split(\"\\t\")\n",
        "        col_idx = {h: i for i, h in enumerate(header)}\n",
        "\n",
        "        doc, visit_codes, label_seq = [], [], []\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            return docs, labels\n",
        "\n",
        "        tokens = line.strip().split(\"\\t\")\n",
        "        pid, day_id = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "        label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "        while line:\n",
        "            tokens = line.strip().split(\"\\t\")\n",
        "            c_pid, c_day = tokens[col_idx[\"PID\"]], tokens[col_idx[\"DAY_ID\"]]\n",
        "\n",
        "            if c_pid != pid:\n",
        "                doc.append(visit_codes)\n",
        "                docs.append(doc)\n",
        "                labels.append(label_seq)\n",
        "                # reset\n",
        "                doc, visit_codes, label_seq = [], [], []\n",
        "                pid, day_id = c_pid, c_day\n",
        "                label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "            else:\n",
        "                # same patient, check if new day\n",
        "                if c_day != day_id:\n",
        "                    doc.append(visit_codes)\n",
        "                    visit_codes = []\n",
        "                    day_id = c_day\n",
        "                    label_seq.append(1 if is_readmitted(pid, day_id) else 0)\n",
        "\n",
        "            diag_str = tokens[col_idx[\"DX_GROUP_DESCRIPTION\"]]\n",
        "            diag_idx = word2idx.get(diag_str, config.unknown_index)\n",
        "            visit_codes.append(diag_idx)\n",
        "            line = f.readline()\n",
        "\n",
        "        # finalize\n",
        "        doc.append(visit_codes)\n",
        "        docs.append(doc)\n",
        "        labels.append(label_seq)\n",
        "\n",
        "    return docs, labels\n",
        "\n",
        "def split_and_save(config: Config, docs, labels):\n",
        "    \"\"\"\n",
        "    Splits docs/labels => train/valid/test and saves as pkl.\n",
        "    \"\"\"\n",
        "    # adjust these splits as needed\n",
        "    train_end = 2000\n",
        "    val_end = 2500\n",
        "\n",
        "    save_pkl(config.pkl_train_x, docs[:train_end])\n",
        "    save_pkl(config.pkl_train_y, labels[:train_end])\n",
        "\n",
        "    save_pkl(config.pkl_val_x, docs[train_end:val_end])\n",
        "    save_pkl(config.pkl_val_y, labels[train_end:val_end])\n",
        "\n",
        "    save_pkl(config.pkl_test_x, docs[val_end:])\n",
        "    save_pkl(config.pkl_test_y, labels[val_end:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKvHSFILcv8"
      },
      "source": [
        "### PyTorch Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3a7gCxvtLgJt"
      },
      "outputs": [],
      "source": [
        "class PatientVisitsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset that holds (docs, labels) for a split (train/valid/test).\n",
        "    We'll convert them to multi-hot within __getitem__ or in a collate_fn.\n",
        "    \"\"\"\n",
        "    def __init__(self, docs, labels, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.docs = docs\n",
        "        self.labels = labels\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.docs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.docs[idx], self.labels[idx]\n",
        "\n",
        "def multi_hot_collate_fn(batch, vocab_size, max_len):\n",
        "    \"\"\"\n",
        "    Collate function to transform a list of (doc, label) => (x, y, mask).\n",
        "    Each doc is a list of visits.\n",
        "    We'll multi-hot each visit, up to max_len visits.\n",
        "    \"\"\"\n",
        "    # We'll first figure out batch_size from the list\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # 1) separate docs and labels\n",
        "    docs = [b[0] for b in batch]\n",
        "    labels = [b[1] for b in batch]\n",
        "\n",
        "    # 2) find the truncated length\n",
        "    truncated_docs = []\n",
        "    truncated_labels = []\n",
        "    lengths = []\n",
        "    for doc, lab in zip(docs, labels):\n",
        "        if len(doc) <= max_len:\n",
        "            truncated_docs.append(doc)\n",
        "            truncated_labels.append(lab)\n",
        "            lengths.append(len(doc))\n",
        "        else:\n",
        "            truncated_docs.append(doc[:max_len])\n",
        "            truncated_labels.append(lab[:max_len])\n",
        "            lengths.append(max_len)\n",
        "\n",
        "    max_len_batch = max(lengths) if lengths else 0\n",
        "    # 3) Allocate arrays\n",
        "    x_array = np.zeros((batch_size, max_len_batch, vocab_size), dtype=np.float32)\n",
        "    y_array = np.ones((batch_size, max_len_batch), dtype=np.float32)\n",
        "    mask_array = np.zeros((batch_size, max_len_batch), dtype=np.float32)\n",
        "\n",
        "    # 4) Fill\n",
        "    for i, (doc, lab) in enumerate(zip(truncated_docs, truncated_labels)):\n",
        "        seq_len = len(doc)\n",
        "        mask_array[i, :seq_len] = 1\n",
        "        y_array[i, :seq_len] = lab\n",
        "        for j, visit_codes in enumerate(doc):\n",
        "            for code_idx in visit_codes:\n",
        "                x_array[i, j, code_idx-1] = 1\n",
        "\n",
        "    # 5) Convert to torch\n",
        "    x_tensor = torch.from_numpy(x_array)\n",
        "    y_tensor = torch.from_numpy(y_array)\n",
        "    mask_tensor = torch.from_numpy(mask_array)\n",
        "    return x_tensor, y_tensor, mask_tensor\n",
        "\n",
        "def create_dataloader(docs, labels, config: Config, shuffle=False):\n",
        "    \"\"\"\n",
        "    Convenience method to build a DataLoader from docs/labels.\n",
        "    \"\"\"\n",
        "    dataset = PatientVisitsDataset(docs, labels, config.vocab_size, config.max_visit_len)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=lambda b: multi_hot_collate_fn(b, config.vocab_size, config.max_visit_len),\n",
        "    )\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Py4FclNbtw"
      },
      "source": [
        "### EXECUTE Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nJFXK01cNhXr"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../resource/S1_Data.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Build vocab & Load it\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m w2i \u001b[38;5;241m=\u001b[39m load_vocab_dict(config)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2) Extract events => docs => split => pkl\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_vocab\u001b[39m(config: Config):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    Creates vocab.txt and stop.txt from S1_Data.txt by filtering.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Index offset so that 'unknown_index' can be used.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDX_GROUP_DESCRIPTION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Filter out rare\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/anaconda3/envs/content-env/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../resource/S1_Data.txt'"
          ]
        }
      ],
      "source": [
        "# 1) Build vocab & Load it\n",
        "build_vocab(config)\n",
        "w2i = load_vocab_dict(config)\n",
        "\n",
        "# 2) Extract events => docs => split => pkl\n",
        "inpat_events = extract_inpatient_events(config)\n",
        "docs, labels = convert_format(config, w2i, inpat_events)\n",
        "split_and_save(config, docs, labels)\n",
        "\n",
        "# 3) Load those splits into memory\n",
        "X_train = load_pkl(config.pkl_train_x)\n",
        "Y_train = load_pkl(config.pkl_train_y)\n",
        "\n",
        "X_valid = load_pkl(config.pkl_val_x)\n",
        "Y_valid = load_pkl(config.pkl_val_y)\n",
        "\n",
        "X_test  = load_pkl(config.pkl_test_x)\n",
        "Y_test  = load_pkl(config.pkl_test_y)\n",
        "\n",
        "# 4) Create DataLoaders\n",
        "train_loader = create_dataloader(X_train, Y_train, config, shuffle=True)\n",
        "valid_loader = create_dataloader(X_valid, Y_valid, config, shuffle=False)\n",
        "test_loader  = create_dataloader(X_test,  Y_test,  config, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6umc8Ym6Li_d"
      },
      "source": [
        "### Defining CONTENT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUAIzhoILlv7"
      },
      "outputs": [],
      "source": [
        "class ThetaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Reparameterization for the topic distribution + KL term.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_len, n_topics):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.klterm = 0.0\n",
        "        self.theta = None\n",
        "        self.n_topics = n_topics\n",
        "\n",
        "    def forward(self, mu, log_sigma):\n",
        "        eps = torch.randn_like(mu)\n",
        "        z = mu + torch.exp(0.5 * log_sigma) * eps\n",
        "        theta = F.softmax(z, dim=1)\n",
        "        self.theta = theta\n",
        "\n",
        "        # kl\n",
        "        self.klterm = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.exp())\n",
        "\n",
        "        # Expand => [batch_size, seq_len, n_topics]\n",
        "        expanded_theta = theta.unsqueeze(1).expand(-1, self.max_len, self.n_topics)\n",
        "        return expanded_theta\n",
        "\n",
        "\n",
        "class ContentModel(nn.Module):\n",
        "    \"\"\"\n",
        "    RNN + Topic model:\n",
        "      1) embed => GRU\n",
        "      2) dense => mu, log_sigma => Theta\n",
        "      3) B * Theta => context\n",
        "      4) sum => final prediction\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed_size = config.embed_size\n",
        "        self.hidden_size= config.hidden_size\n",
        "        self.n_topics   = config.n_topics\n",
        "        self.max_len    = config.max_visit_len\n",
        "\n",
        "        self.embed = nn.Linear(self.vocab_size, self.embed_size, bias=False)\n",
        "        self.gru = nn.GRU(self.embed_size, self.hidden_size, batch_first=True)\n",
        "\n",
        "        self.dense1 = nn.Linear(self.vocab_size, self.hidden_size)\n",
        "        self.dense2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.mu = nn.Linear(self.hidden_size, self.n_topics)\n",
        "        self.log_sigma = nn.Linear(self.hidden_size, self.n_topics)\n",
        "\n",
        "        self.B = nn.Linear(self.vocab_size, self.n_topics, bias=False)\n",
        "        self.out_layer = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "        self.theta_layer = ThetaLayer(self.max_len, self.n_topics)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x => [batch, seq_len, vocab_size]\n",
        "        embedded = self.embed(x) * mask.unsqueeze(-1)  # [B, T, embed_size]\n",
        "        gru_out, h_n = self.gru(embedded)\n",
        "\n",
        "        # topic\n",
        "        h1 = F.relu(self.dense1(x))  # [B, T, hidden]\n",
        "        h2 = F.relu(self.dense2(h1)) # [B, T, hidden]\n",
        "\n",
        "        avg_h2 = h2.mean(dim=1)      # [B, hidden]\n",
        "        mu_val = self.mu(avg_h2)\n",
        "        log_sigma_val = self.log_sigma(avg_h2)\n",
        "\n",
        "        theta_expanded = self.theta_layer(mu_val, log_sigma_val)  # [B, T, n_topics]\n",
        "\n",
        "        # context\n",
        "        B_out = self.B(x)  # [B, T, n_topics]\n",
        "        context = (B_out * theta_expanded).mean(dim=-1)  # [B, T]\n",
        "\n",
        "        # combine\n",
        "        rnn_scores = self.out_layer(gru_out).squeeze(-1)  # [B, T]\n",
        "        logits = rnn_scores + context\n",
        "        out = torch.sigmoid(logits)\n",
        "        out = out * mask + 1e-6  # mask out\n",
        "        return out, h_n, self.theta_layer.theta\n",
        "\n",
        "    @property\n",
        "    def kl_term(self):\n",
        "        return self.theta_layer.klterm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrvlN6aYNpHz"
      },
      "source": [
        "### EXECUTE Model Initation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cmyn5qeNr6d"
      },
      "outputs": [],
      "source": [
        "# 5) Build Model\n",
        "model = ContentModel(config).to(config.device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S93_mShgLnX3"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAA6PVXgLvU4"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, config: Config):\n",
        "    \"\"\"\n",
        "    Train step over 'loader' => returns average train loss, plus any collected [theta+hidden].\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    batch_count= 0\n",
        "    collector  = []\n",
        "\n",
        "    for x_batch, y_batch, m_batch in loader:\n",
        "        x_batch = x_batch.to(config.device)\n",
        "        y_batch = y_batch.to(config.device)\n",
        "        m_batch = m_batch.to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds, h_n, theta = model(x_batch, m_batch)\n",
        "\n",
        "        bce = F.binary_cross_entropy(preds.view(-1), y_batch.view(-1), reduction=\"sum\")\n",
        "        loss= bce + model.kl_term\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batch_count+= 1\n",
        "\n",
        "        # store [theta + hidden]\n",
        "        rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "        theta_np= theta.detach().cpu().numpy()\n",
        "        combined= np.concatenate([theta_np, rnn_vec], axis=1)\n",
        "        collector.append(combined)\n",
        "\n",
        "    avg_loss = total_loss / max(batch_count,1)\n",
        "    return avg_loss, collector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSlY03-eX4i"
      },
      "source": [
        "### EXECUTE Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB-j36hZX7YA"
      },
      "outputs": [],
      "source": [
        "for epoch in range(config.num_epochs):\n",
        "  st = time.time()\n",
        "\n",
        "  # --- Train on train set ---\n",
        "  train_loss, train_theta_collector = train(model, train_loader, optimizer, config)\n",
        "  train_thetas_arr = np.concatenate(train_theta_collector, axis=0)\n",
        "  np.save(os.path.join(config.theta_dir, f\"thetas_train_{epoch}.npy\"), train_thetas_arr)\n",
        "\n",
        "  elapsed = time.time() - st\n",
        "  print(f\"\\nEpoch {epoch+1}/{config.num_epochs} took {elapsed:.2f}s\")\n",
        "  print(f\"  [Train] loss={train_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BxQk_7SLxxA"
      },
      "source": [
        "### Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdEc8wHmL2pp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader, config: Config):\n",
        "    \"\"\"\n",
        "    Evaluates on a DataLoader (e.g. test/valid).\n",
        "    Returns (avg_loss, list_of_true, list_of_pred, theta_hidden_collector).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    batch_count= 0\n",
        "    all_true, all_pred = [], []\n",
        "    all_theta_hidden = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch, mask_batch in loader:\n",
        "            x_batch = x_batch.to(config.device)\n",
        "            y_batch = y_batch.to(config.device)\n",
        "            mask_batch = mask_batch.to(config.device)\n",
        "\n",
        "            preds, h_n, theta = model(x_batch, mask_batch)\n",
        "            bce = F.binary_cross_entropy(preds.view(-1), y_batch.view(-1), reduction=\"sum\")\n",
        "            loss= bce + model.kl_term\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # flatten predictions/labels ignoring masked positions\n",
        "            seq_lens = mask_batch.sum(dim=1).cpu().numpy().astype(int)\n",
        "            preds_np = preds.detach().cpu().numpy()\n",
        "            y_np     = y_batch.detach().cpu().numpy()\n",
        "\n",
        "            for i in range(x_batch.shape[0]):\n",
        "                length_i = seq_lens[i]\n",
        "                all_pred.extend(preds_np[i, :length_i])\n",
        "                all_true.extend(y_np[i, :length_i])\n",
        "\n",
        "            # store [theta + hidden]\n",
        "            rnn_vec = h_n.squeeze(0).detach().cpu().numpy()\n",
        "            theta_np= theta.detach().cpu().numpy()\n",
        "            combined= np.concatenate([theta_np, rnn_vec], axis=1)\n",
        "            all_theta_hidden.append(combined)\n",
        "\n",
        "    avg_loss = total_loss / (batch_count if batch_count else 1)\n",
        "    return avg_loss, all_true, all_pred, all_theta_hidden\n",
        "\n",
        "\n",
        "def compute_metrics(true_vals, pred_vals):\n",
        "    \"\"\"\n",
        "    Returns a dict with AUC, PR-AUC, ACC, Precision, Recall, F1.\n",
        "    \"\"\"\n",
        "    auc_val = roc_auc_score(true_vals, pred_vals)\n",
        "    pr_val  = pr_auc(true_vals, pred_vals)\n",
        "    preds_bin = (np.array(pred_vals) > 0.5).astype(int)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(true_vals, preds_bin, average=\"binary\")\n",
        "    acc_val = accuracy_score(true_vals, preds_bin)\n",
        "    return {\n",
        "        \"auc\": auc_val,\n",
        "        \"prauc\": pr_val,\n",
        "        \"acc\": acc_val,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub5jcT6Mefqb"
      },
      "source": [
        "### EXECUTE Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DevIGIDCdx79"
      },
      "outputs": [],
      "source": [
        "# Step F: final test (completely separate from training loop)\n",
        "test_loss, test_true, test_pred, test_theta_collector = evaluate_model(model, test_loader, config)\n",
        "test_metrics = compute_metrics(test_true, test_pred)\n",
        "\n",
        "# --- Validate on val set (optional each epoch) ---\n",
        "val_loss, val_true, val_pred, val_theta_collector = evaluate_model(model, valid_loader, config)\n",
        "val_metrics = compute_metrics(val_true, val_pred)\n",
        "\n",
        "# Save the test results\n",
        "test_thetas_arr = np.concatenate(test_theta_collector, axis=0)\n",
        "np.save(os.path.join(config.theta_dir,  f\"thetas_test_final.npy\"), test_thetas_arr)\n",
        "np.save(os.path.join(config.results_dir,f\"test_labels_final.npy\"), np.array(test_true))\n",
        "np.save(os.path.join(config.results_dir,f\"test_preds_final.npy\"),  np.array(test_pred))\n",
        "\n",
        "print(f\"[Test] loss={test_loss:.4f}, AUC={test_metrics['auc']:.4f}, \"\n",
        "      f\"PR-AUC={test_metrics['prauc']:.4f}, ACC={test_metrics['acc']:.4f}, \"\n",
        "      f\"Precision={test_metrics['precision']:.4f}, Recall={test_metrics['recall']:.4f}, \"\n",
        "      f\"F1={test_metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"[Valid] loss={val_loss:.4f}, AUC={val_metrics['auc']:.4f}, PR-AUC={val_metrics['prauc']:.4f}, \"\n",
        "        f\"ACC={val_metrics['acc']:.4f}, Precision={val_metrics['precision']:.4f}, \"\n",
        "        f\"Recall={val_metrics['recall']:.4f}, F1={val_metrics['f1']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "content-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
